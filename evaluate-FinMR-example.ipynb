{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea2ac4-fa3b-4ab5-8b78-44ada0d7475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.std import tqdm\n",
    "import re\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6114402-c8a0-4658-a82f-b2394ba11a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e808f5-5f53-46ea-8ec1-9c2969ad42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(result_path, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        json_line = json.loads(line)\n",
    "        data.append(json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf698bfd-8dc3-4408-b559-cac0ab8131e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(json_data):\n",
    "    true_answer = []\n",
    "    pred_answer = []\n",
    "\n",
    "    ## for the results generated from lm-evaluation-harness framework\n",
    "    for json_line in tqdm(json_data): \n",
    "        target = json_line.get(\"target\")\n",
    "        resps = json_line.get(\"filtered_resps\")[0]\n",
    "        true_answer.append(target)\n",
    "        pred_answer.append(resps)\n",
    "\n",
    "    return true_answer, pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c53d2-1420-455e-943f-626baaf55988",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_answer, pred_answer = parse_result(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584073b3-862d-4fa2-8d5f-55f64a17445c",
   "metadata": {},
   "source": [
    "### llm-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd319b40-0ecd-4ebe-93cf-dc3ed53874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961bb29-3ec0-40ac-aa9d-df254f24393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cb8e8-4180-410c-9586-68542f1702b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfc7c4-49ec-4d3a-8a05-d9b27d5901a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(true_answer, pred_answer):\n",
    "    return f\"\"\"Instruction: You are an evaluator. Your task is to judge whether the modelâ€™s output pred_answer is correct compared to the given true_answer. \n",
    "Follow the rules strictly:\n",
    "\n",
    "Step 1 (Structure Check):\n",
    "    Verify whether pred_answer has the same structure as true_answer. The required structure is a JSON object with exactly two keys:\n",
    "        {{\"extracted_value\": <value>, \"calculated_value\": <value>}}\n",
    "    Minor formatting differences (e.g., line breaks, indentation, whitespace) are acceptable.\n",
    "    If the structure is invalid, output the label: S\n",
    "    If valid, continue to Step 2\n",
    "\n",
    "Step 2 (Extracted Value Check):\n",
    "    Compare true_answer[\"extracted_value\"] and pred_answer[\"extracted_value\"] by their mathematical meaning, not their string form. For example, \"-1,284\" and \"-1284\" are considered equal.\n",
    "    If they are not equal in numeric meaning, output the label: E\n",
    "    If equal, continue to Step 3\n",
    "\n",
    "Step 3 (Calculated Value Check):\n",
    "    Compare true_answer[\"calculated_value\"] and pred_answer[\"calculated_value\"] strictly in numeric meaning. They must be exactly equal (zero tolerance).\n",
    "    If they are not equal, output the label: C\n",
    "    If equal, then everything is correct\n",
    "\n",
    "Final Decision:\n",
    "    If all three checks pass, output the label: A\n",
    "Output only one label: S, E, C, or A. Do not explain your reasoning.\n",
    "\n",
    "Example 1:\n",
    "    true_answer = {{\"extracted_value\": \"-1,286\", \"calculated_value\": \"1,286\"}}\n",
    "    pred_answer = {{\"extracted_value\": \"-1286\", \"calculated_value\": \"0\"}}\n",
    "\n",
    "    Output: C\n",
    "\n",
    "Example 2:\n",
    "    true_answer = {{\"extracted_value\": \"5,000\", \"calculated_value\": \"5,000\"}}\n",
    "    pred_answer = {{\"extracted_value\": \"5000\", \"calculated_value\": \"5000\"}}\n",
    "\n",
    "    Output: A\n",
    "\n",
    "Example 3:\n",
    "    true_answer = {{\"extracted_value\": \"123\", \"calculated_value\": \"456\"}}\n",
    "    pred_answer = {{\"extracted_value\": \"124\", \"calculated_value\": \"456\"}}\n",
    "\n",
    "    Output: E\n",
    "\n",
    "Example 4:\n",
    "    true_answer = {{\"extracted_value\": \"100\", \"calculated_value\": \"200\"}}\n",
    "    pred_answer = {{\"wrong_key\": \"100\", \"calculate_value\": \"200\"}}\n",
    "\n",
    "    Output: S\n",
    "\n",
    "Input:\n",
    "    true_answer = {true_answer}\n",
    "    pred_answer = {pred_answer}\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a055768-1ca4-40e1-bb13-3438426e448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=user_input,\n",
    "        reasoning={\n",
    "            \"effort\": \"minimal\"\n",
    "        },\n",
    "        text={\n",
    "            \"verbosity\": \"low\"\n",
    "        }\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf115d7a-4040-498e-ade3-09a0c4c19ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_LABELS = {\"A\", \"S\", \"E\", \"C\"}\n",
    "\n",
    "def evaluate_performance(true_answer: List[str], pred_answer: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Based on the output of LLM-as-a-judge (A/S/E/C), calculate:\n",
    "    - Parsing success rate (percentage of samples evaluated)\n",
    "    - Accuracy rate (percentage of samples A evaluated)\n",
    "    - Structural error rate (percentage of samples S evaluated)\n",
    "    - Extraction error rate (percentage of samples E evaluated)\n",
    "    - Calculation error rate (percentage of samples C evaluated)\n",
    "    \n",
    "    Requires external definitions:\n",
    "    - get_prompt(t_a, p_a) -> str\n",
    "    - get_response(user_input) -> str | Any object that can be converted to a string\n",
    "    \"\"\"\n",
    "    if len(true_answer) != len(pred_answer):\n",
    "        # To avoid silent truncation, the shorter length is selected and prompted\n",
    "        n = min(len(true_answer), len(pred_answer))\n",
    "        ta_iter = true_answer[:n]\n",
    "        pa_iter = pred_answer[:n]\n",
    "    else:\n",
    "        n = len(true_answer)\n",
    "        ta_iter = true_answer\n",
    "        pa_iter = pred_answer\n",
    "\n",
    "    A_list, S_list, E_list, C_list = [], [], [], []\n",
    "    output_errors = []\n",
    "\n",
    "    for t_a, p_a in tqdm(zip(ta_iter, pa_iter), total=n, desc=\"Evaluating\"):\n",
    "        user_input = get_prompt(t_a, p_a)\n",
    "        res_raw = get_response(user_input)\n",
    "        # Normalization judgment: only accept single characters and within {A,S,E,C}\n",
    "        res = str(res_raw).strip().upper()\n",
    "        if res in VALID_LABELS and len(res) == 1:\n",
    "            if res == \"A\":\n",
    "                A_list.append(res)\n",
    "            elif res == \"S\":\n",
    "                S_list.append(res)\n",
    "            elif res == \"E\":\n",
    "                E_list.append(res)\n",
    "            elif res == \"C\":\n",
    "                C_list.append(res)\n",
    "        else:\n",
    "            output_errors.append(res_raw)\n",
    "\n",
    "    total = n\n",
    "    evaluated = total - len(output_errors)\n",
    "\n",
    "    def pct(x, d):\n",
    "        return round(100.0 * x / d, 2) if d > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        # Parsing success rate: The proportion of samples that were successfully parsed and participated in the evaluation to the total samples\n",
    "        \"Parsing success rate(%)\": pct(evaluated, total),\n",
    "\n",
    "        # The denominators of the following four items are evaluated (the number of samples actually evaluated)\n",
    "        \"ACC(%)\": pct(len(A_list), evaluated),\n",
    "        \"Structural error rate(%)\": pct(len(S_list), evaluated),\n",
    "        \"Extraction error rate(%)\": pct(len(E_list), evaluated),\n",
    "        \"Calculating error rate(%)\": pct(len(C_list), evaluated),\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62c0a6-f915-4ff8-a250-cf7dcf86d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(true_answer, pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a96796-6369-4694-b2ac-3732bbf37a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter-citation",
   "language": "python",
   "name": "my-jupyter-citation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
