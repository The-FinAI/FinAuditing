{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea2ac4-fa3b-4ab5-8b78-44ada0d7475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.std import tqdm\n",
    "import re\n",
    "from typing import List\n",
    "import codecs\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6114402-c8a0-4658-a82f-b2394ba11a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e808f5-5f53-46ea-8ec1-9c2969ad42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(result_path, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        json_line = json.loads(line)\n",
    "        data.append(json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b49b19-8004-40c2-9219-1918acc7c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_code_block(text: str):\n",
    "    \"\"\"\n",
    "    Remove surrounding triple backtick code block markers and parse into a Python object.\n",
    "    Handles escaped JSON strings as a special case.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # strip ```lang ... ``` fences\n",
    "    if cleaned.startswith(\"```\") and cleaned.endswith(\"```\"):\n",
    "        cleaned = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", cleaned, flags=re.DOTALL)\n",
    "        cleaned = re.sub(r\"\\s*```$\", \"\", cleaned, flags=re.DOTALL).strip()\n",
    "\n",
    "    # 1) plain JSON\n",
    "    try:\n",
    "        out = json.loads(cleaned)\n",
    "        # if the decoded result is itself a JSON-looking string, decode again\n",
    "        if isinstance(out, str) and out.strip().startswith((\"[\", \"{\")):\n",
    "            try:\n",
    "                return json.loads(out)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return out\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 2) escaped JSON like: [\\\"a\\\", \\\"b\\\"]\n",
    "    try:\n",
    "        unescaped = codecs.decode(cleaned, \"unicode_escape\")\n",
    "        out = json.loads(unescaped)\n",
    "        return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Python-literal-style list/dict like: ['a', 'b']\n",
    "    try:\n",
    "        return ast.literal_eval(cleaned)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) last resort: simple de-escape of \\\" then try JSON again\n",
    "    try:\n",
    "        return json.loads(cleaned.replace('\\\\\"', '\"'))\n",
    "    except Exception:\n",
    "        return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf698bfd-8dc3-4408-b559-cac0ab8131e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for the results generated from lm-evaluation-harness framework\n",
    "def parse_result(json_data):\n",
    "    true_answer = []\n",
    "    pred_answer = []\n",
    "    \n",
    "    for json_line in tqdm(json_data): \n",
    "        target = eval(json.loads(json_line.get(\"target\")))\n",
    "        resps = parse_code_block(json_line.get(\"filtered_resps\")[0])\n",
    "        true_answer.append(target)\n",
    "        pred_answer.append(resps)\n",
    "\n",
    "    return true_answer, pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c53d2-1420-455e-943f-626baaf55988",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_answer, pred_answer = parse_result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824a3b5-67a2-48e0-9d5b-960842f6666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate_at_k(true_answer: List[List[str]], pred_answer: List[List[str]], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute HR@k (Hit Rate at k).\n",
    "    true_answer[i] : list of ground truth elements for query i\n",
    "    pred_answer[i] : list of predicted elements for query i (ranked list)\n",
    "    \"\"\"\n",
    "    assert len(true_answer) == len(pred_answer), \"true_answer and pred_answer must have the same length\"\n",
    "    N = len(true_answer)\n",
    "    hits = 0\n",
    "    for g, p in zip(true_answer, pred_answer):\n",
    "        topk_preds = set(p[:k])\n",
    "        if topk_preds & set(g):  # intersection is not empty\n",
    "            hits += 1\n",
    "    return hits / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd999b0-d874-490f-9a9c-bd9633228c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(true_answer: List[List[str]], pred_answer: List[List[str]], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute R@k (Recall at k).\n",
    "    \"\"\"\n",
    "    assert len(true_answer) == len(pred_answer), \"true_answer and pred_answer must have the same length\"\n",
    "    N = len(true_answer)\n",
    "    recall_sum = 0\n",
    "    for g, p in zip(true_answer, pred_answer):\n",
    "        if len(g) == 0:  # avoid division by zero\n",
    "            continue\n",
    "        topk_preds = set(p[:k])\n",
    "        recall_sum += len(topk_preds & set(g)) / len(g)\n",
    "    return recall_sum / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452d943-a457-4524-a3bd-6948613287bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_at_k(true_answer: List[List[str]],\n",
    "                  pred_answer: List[List[str]],\n",
    "                  k: int,\n",
    "                  zero_division: float = 0.0,\n",
    "                  ignore_empty_gold: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compute Macro F1@k.\n",
    "    - true_answer[i]: ground-truth elements for query i\n",
    "    - pred_answer[i]: ranked predictions for query i\n",
    "    - k: use top-k predictions\n",
    "    - zero_division: value used when precision or recall denominator is zero\n",
    "    - ignore_empty_gold: if True, skip queries with empty gold set\n",
    "    \"\"\"\n",
    "    assert len(true_answer) == len(pred_answer), \"Lengths must match.\"\n",
    "\n",
    "    f1_sum = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for g, p in zip(true_answer, pred_answer):\n",
    "        G = set(g)\n",
    "        if ignore_empty_gold and len(G) == 0:\n",
    "            continue\n",
    "\n",
    "        P = set(p[:k])\n",
    "\n",
    "        tp = len(G & P)\n",
    "        fp = len(P - G)\n",
    "        fn = len(G - P)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else zero_division\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else zero_division\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        f1_sum += f1\n",
    "        n += 1\n",
    "\n",
    "    return (f1_sum / n) if n > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b0ebc-fd94-4521-b325-d0ff32c3f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(true_answer, pred_answer, ks=[1, 5, 10, 20]):\n",
    "    results = {}\n",
    "    hr_list, r_list, f1_list = [], [], []\n",
    "\n",
    "    for k in ks:\n",
    "        hr = hit_rate_at_k(true_answer, pred_answer, k) * 100\n",
    "        r = recall_at_k(true_answer, pred_answer, k) * 100\n",
    "        macro_f1 = macro_f1_at_k(true_answer, pred_answer, k) * 100\n",
    "\n",
    "        results[f\"HR@{k}\"] = f\"{hr:.2f}%\"\n",
    "        results[f\"R@{k}\"] = f\"{r:.2f}%\"\n",
    "        results[f\"Macro-F1@{k}\"] = f\"{macro_f1:.2f}%\"\n",
    "\n",
    "        hr_list.append(hr)\n",
    "        r_list.append(r)\n",
    "        f1_list.append(macro_f1)\n",
    "\n",
    "    results[\"HR@avg\"] = f\"{sum(hr_list) / len(hr_list):.2f}%\"\n",
    "    results[\"R@avg\"] = f\"{sum(r_list) / len(r_list):.2f}%\"\n",
    "    results[\"Macro-F1@avg\"] = f\"{sum(f1_list) / len(f1_list):.2f}%\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed39ff-0117-4257-a8ba-c7a831d01d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(true_answer, pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e356be-d6e0-488f-8c4e-e3fb02a90888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter-citation",
   "language": "python",
   "name": "my-jupyter-citation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
