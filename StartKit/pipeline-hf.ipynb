{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29097b-7acc-44e3-a2a9-a37699a456ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d062fc-95ca-4d30-938a-541d0eae18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATASET_NAME = \"TheFinAI/FinSM\"\n",
    "SPLIT = \"test\"          \n",
    "QUERY_COL = \"query\"     \n",
    "ID_COL = \"id\"\n",
    "GT_COL = \"answer\"\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "OUT_PATH = \"predictions-hf.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88d3b1-b45b-47c8-a282-c646b352154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "DO_SAMPLE = TEMPERATURE > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df41af-a404-42f3-bef6-05a3c24dfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input handling\n",
    "MAX_INPUT_TOKENS = 90000   # If your query is long, set the value to a larger number, but it must be less than or equal to the value supported by the model.\n",
    "TRUNCATION = True\n",
    "\n",
    "# memory options\n",
    "USE_4BIT = False           # True: 4-bit quantization (saves more VRAM, but may result in slightly slower speed/reduced quality)\n",
    "DTYPE = torch.bfloat16     # If the GPU supports bf16, use bf16; otherwise, use torch.float16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3730355-0c3c-4d07-9b7b-b43d62bfddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load dataset\n",
    "# =========================\n",
    "ds = load_dataset(DATASET_NAME, split=SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da1951-e93a-4da7-8254-7c6efc573cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load tokenizer\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39915ec0-6b89-4c98-84c9-6adace5e1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load model (70B-friendly)\n",
    "# =========================\n",
    "model_kwargs: Dict[str, Any] = dict(\n",
    "    torch_dtype=DTYPE if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,  # Automatically assigned to multiple cards/single card\n",
    ")\n",
    "\n",
    "if USE_4BIT:\n",
    "    # bitsandbytes\n",
    "    model_kwargs.update(dict(load_in_4bit=True))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace709b3-7017-4d01-91f9-91cb423113f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Inference loop\n",
    "# =========================\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, ex in enumerate(tqdm(ds, desc=\"Infer\")):\n",
    "        query = ex.get(QUERY_COL, \"\")\n",
    "        ex_id = ex.get(ID_COL, i)\n",
    "        gt = ex.get(GT_COL, None)\n",
    "\n",
    "        chat_prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": query}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True, \n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            chat_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=TRUNCATION,\n",
    "            max_length=MAX_INPUT_TOKENS,\n",
    "        )\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # when device_map=\"auto\", you can put inputs on cuda:0 safely\n",
    "        # (HF will dispatch internally). If you're on CPU, keep as is.\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                temperature=TEMPERATURE if DO_SAMPLE else None,\n",
    "                top_p=TOP_P,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # decode full text\n",
    "        full_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        # OPTIONAL: extract only the newly generated portion (common for eval)\n",
    "        # This avoids including the prompt again in \"prediction\"\n",
    "        prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "        gen_ids = out[0][prompt_len:]\n",
    "        prediction = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        record = {\n",
    "            \"id\": ex_id,\n",
    "            \"prediction\": prediction,\n",
    "            \"ground_truth\": gt,\n",
    "        }\n",
    "\n",
    "        # Optional: Retain the query for easier debugging.\n",
    "        record[\"query\"] = query\n",
    "\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved: {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca2451-22a3-471d-a374-48cee7428838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
