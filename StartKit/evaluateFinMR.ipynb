{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe09b18-bfbf-4a39-b042-a3feec617c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928b00f-f6e1-417b-a52f-9f4e95a92805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0) Config\n",
    "# -------------------------\n",
    "LABELS = {\"A\", \"S\", \"E\", \"C\"}\n",
    "PRED_KEY = \"prediction\"\n",
    "GOLD_KEY = \"ground_truth\"\n",
    "\n",
    "# Optional: Write out the judge results for easier reproduction/debugging later.\n",
    "JUDGE_OUT_JSONL = \"finmr_judge_results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3aed4c-d53a-4204-bf24-57d96c64a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1) Robust parsing helpers\n",
    "# -------------------------\n",
    "\n",
    "_CODEBLOCK_RE = re.compile(r\"^```[a-zA-Z0-9]*\\n([\\s\\S]*?)\\n```$\", re.M)\n",
    "_JSON_OBJ_RE = re.compile(r\"\\{[\\s\\S]*\\}\")  # greedy match the last '}' (often ok)\n",
    "\n",
    "def _strip_codeblock(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    m = _CODEBLOCK_RE.search(s)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def parse_json_object_best_effort(text: Any) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Try to extract a JSON object from text.\n",
    "    Returns dict if possible, else None.\n",
    "\n",
    "    Handles:\n",
    "      - already dict\n",
    "      - JSON string\n",
    "      - text containing {...}\n",
    "      - code block wrapping\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    if isinstance(text, dict):\n",
    "        return text\n",
    "    if isinstance(text, str):\n",
    "        s = text.strip()\n",
    "    else:\n",
    "        s = str(text).strip()\n",
    "\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    s = _strip_codeblock(s)\n",
    "\n",
    "    # direct parse\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "            return obj if isinstance(obj, dict) else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # extract {...} from longer text\n",
    "    m = _JSON_OBJ_RE.search(s)\n",
    "    if m:\n",
    "        obj_str = m.group(0).strip()\n",
    "        try:\n",
    "            obj = json.loads(obj_str)\n",
    "            return obj if isinstance(obj, dict) else None\n",
    "        except Exception:\n",
    "            # common minor issue: single quotes\n",
    "            try:\n",
    "                obj = json.loads(obj_str.replace(\"'\", '\"'))\n",
    "                return obj if isinstance(obj, dict) else None\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def normalize_two_key_obj(obj: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Ensure object has exactly two required keys:\n",
    "      extracted_value, calculated_value\n",
    "    Return normalized dict if valid else None.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "    if set(obj.keys()) != {\"extracted_value\", \"calculated_value\"}:\n",
    "        return None\n",
    "    return {\n",
    "        \"extracted_value\": obj[\"extracted_value\"],\n",
    "        \"calculated_value\": obj[\"calculated_value\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399bc94-6854-4f0a-93fa-eeb3b1258f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Load from predictions.jsonl\n",
    "# -------------------------\n",
    "\n",
    "def load_finmr_pairs_from_jsonl(path: str) -> Tuple[List[Dict[str, Any]], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - items: list of raw line objects (for id/query etc.)\n",
    "      - true_answer: list of JSON-serializable true objects (dict or str)\n",
    "      - pred_answer: list of raw prediction strings\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    true_answer = []\n",
    "    pred_answer = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            items.append(obj)\n",
    "            true_answer.append(obj.get(GOLD_KEY))\n",
    "            pred_answer.append(obj.get(PRED_KEY, \"\"))\n",
    "    return items, true_answer, pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327b9da-6950-4dd3-9b6b-65db7705957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) LLM-as-a-judge prompt \n",
    "# -------------------------\n",
    "\n",
    "def get_prompt(true_answer: Dict[str, Any], pred_answer: str) -> str:\n",
    "    return f\"\"\"Instruction: You are an evaluator. Your task is to judge whether the model’s output pred_answer is correct compared to the given true_answer. \n",
    "Follow the rules strictly:\n",
    "\n",
    "Step 1 (Structure Check):\n",
    "    Verify whether pred_answer has the same structure as true_answer. The required structure is a JSON object with exactly two keys:\n",
    "        {{\"extracted_value\": <value>, \"calculated_value\": <value>}}\n",
    "    Minor formatting differences (e.g., line breaks, indentation, whitespace) are acceptable.\n",
    "    If the structure is invalid, output the label: S\n",
    "    If valid, continue to Step 2\n",
    "\n",
    "Step 2 (Extracted Value Check):\n",
    "    Compare true_answer[\"extracted_value\"] and pred_answer[\"extracted_value\"] by their mathematical meaning, not their string form. For example, \"-1,284\" and \"-1284\" are considered equal.\n",
    "    If they are not equal in numeric meaning, output the label: E\n",
    "    If equal, continue to Step 3\n",
    "\n",
    "Step 3 (Calculated Value Check):\n",
    "    Compare true_answer[\"calculated_value\"] and pred_answer[\"calculated_value\"] strictly in numeric meaning. They must be exactly equal (zero tolerance).\n",
    "    If they are not equal, output the label: C\n",
    "    If equal, then everything is correct\n",
    "\n",
    "Final Decision:\n",
    "    If all three checks pass, output the label: A\n",
    "Output only one label: S, E, C, or A. Do not explain your reasoning.\n",
    "\n",
    "Input:\n",
    "    true_answer = {true_answer}\n",
    "    pred_answer = {pred_answer}\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f4c14-36c6-40d1-b64a-553207278229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) OpenAI judge wrapper\n",
    "# -------------------------\n",
    "\n",
    "def make_openai_client(api_key: Optional[str] = None):\n",
    "    from openai import OpenAI\n",
    "    api_key = api_key or os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing OPENAI_API_KEY. Set env var or pass api_key=...\")\n",
    "    return OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "def get_response(client,\n",
    "                 user_input: str,\n",
    "                 model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Compatible judge caller for:\n",
    "      - gpt-4o / gpt-4.1  (Chat Completions)\n",
    "      - gpt-5 / gpt-5-mini (Responses API)\n",
    "    Returns plain text.\n",
    "    \"\"\"\n",
    "\n",
    "    model_l = model.lower()\n",
    "\n",
    "    # -----------------------------\n",
    "    # GPT-4o / GPT-4.1 branch\n",
    "    # -----------------------------\n",
    "    if model_l.startswith(\"gpt-4o\") or model_l.startswith(\"gpt-4.1\"):\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a strict evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=4,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    # -----------------------------\n",
    "    # GPT-5 / GPT-5-mini branch\n",
    "    # -----------------------------\n",
    "    else:\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=user_input,\n",
    "            reasoning={\"effort\": \"minimal\"},\n",
    "            text={\"verbosity\": \"low\"},\n",
    "        )\n",
    "        return resp.output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e74f6a-25a0-4ec7-b766-400877a03ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_judge_label(x: Any) -> Optional[str]:\n",
    "    s = str(x).strip().upper()\n",
    "    # Sometimes the model outputs \"A\\n\" or \"Output: A\"; we extract the first A/S/E/C.\n",
    "    if s in LABELS and len(s) == 1:\n",
    "        return s\n",
    "    m = re.search(r\"\\b([ASEC])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcf532-0494-465e-88b9-d19fc0af29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5) Main evaluation\n",
    "# -------------------------\n",
    "\n",
    "def evaluate_finmr_with_judge(\n",
    "    pred_jsonl_path: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    judge_model: str = \"gpt-5-mini\",\n",
    "    save_judge_jsonl: bool = True,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    items, true_raw, pred_raw = load_finmr_pairs_from_jsonl(pred_jsonl_path)\n",
    "    if limit is not None:\n",
    "        items = items[:limit]\n",
    "        true_raw = true_raw[:limit]\n",
    "        pred_raw = pred_raw[:limit]\n",
    "\n",
    "    client = make_openai_client(api_key=api_key)\n",
    "\n",
    "    A_list, S_list, E_list, C_list = [], [], [], []\n",
    "    output_errors = 0\n",
    "    preparse_fail_gold = 0\n",
    "\n",
    "    # Optional: Write the judge's output.\n",
    "    judge_f = open(JUDGE_OUT_JSONL, \"w\", encoding=\"utf-8\") if save_judge_jsonl else None\n",
    "\n",
    "    for obj, t_a_raw, p_a_raw in tqdm(zip(items, true_raw, pred_raw), total=len(items), desc=\"Judging\"):\n",
    "        # First, normalize `true_answer` into a dictionary of the form `dict(extracted_value, calculated_value)`.\n",
    "        t_obj = normalize_two_key_obj(parse_json_object_best_effort(t_a_raw))\n",
    "        if t_obj is None:\n",
    "            # Gold is not legal, and therefore will not be considered for evaluation.\n",
    "            preparse_fail_gold += 1\n",
    "            output_errors += 1\n",
    "            if judge_f:\n",
    "                judge_f.write(json.dumps({\n",
    "                    \"id\": obj.get(\"id\"),\n",
    "                    \"judge_label\": None,\n",
    "                    \"error\": \"invalid_gold_structure\",\n",
    "                    \"ground_truth_raw\": t_a_raw,\n",
    "                    \"prediction_raw\": p_a_raw,\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "            continue\n",
    "\n",
    "        # 直接把 pred 原始文本给 judge（让 judge 做结构/数值校验）\n",
    "        prompt = get_prompt(t_obj, p_a_raw)\n",
    "        res_raw = get_response(client, prompt, model=judge_model)\n",
    "        label = normalize_judge_label(res_raw)\n",
    "\n",
    "        if label is None:\n",
    "            output_errors += 1\n",
    "        else:\n",
    "            if label == \"A\":\n",
    "                A_list.append(1)\n",
    "            elif label == \"S\":\n",
    "                S_list.append(1)\n",
    "            elif label == \"E\":\n",
    "                E_list.append(1)\n",
    "            elif label == \"C\":\n",
    "                C_list.append(1)\n",
    "\n",
    "        if judge_f:\n",
    "            judge_f.write(json.dumps({\n",
    "                \"id\": obj.get(\"id\"),\n",
    "                \"judge_label\": label,\n",
    "                \"judge_raw\": res_raw,\n",
    "                \"ground_truth\": t_obj,\n",
    "                \"prediction_raw\": p_a_raw,\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if judge_f:\n",
    "        judge_f.close()\n",
    "\n",
    "    total = len(items)\n",
    "    evaluated = total - output_errors\n",
    "\n",
    "    def pct(x, d):\n",
    "        return round(100.0 * x / d, 2) if d > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        \"total\": total,\n",
    "        \"evaluated\": evaluated,\n",
    "        \"Parsing success rate(%)\": pct(evaluated, total),\n",
    "\n",
    "        # The denominators of the following four terms are evaluated.\n",
    "        \"ACC(%)\": pct(len(A_list), evaluated),\n",
    "        \"Structural error rate(%)\": pct(len(S_list), evaluated),\n",
    "        \"Extraction error rate(%)\": pct(len(E_list), evaluated),\n",
    "        \"Calculating error rate(%)\": pct(len(C_list), evaluated),\n",
    "\n",
    "        # Additional diagnosis\n",
    "        \"gold_invalid_count\": preparse_fail_gold,\n",
    "        \"judge_output_unparsed_count\": output_errors - preparse_fail_gold,\n",
    "        \"judge_jsonl\": JUDGE_OUT_JSONL if save_judge_jsonl else None,\n",
    "        \"judge_model\": judge_model,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2a837-fdfa-40d2-9725-5f3e1fc7631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = evaluate_finmr_with_judge(\n",
    "#     pred_jsonl_path=\"predictions.jsonl\",\n",
    "#     # api_key=\" ... \",  # Alternatively, you can skip passing the parameter and directly export it as an environment variable: `export OPENAI_API_KEY=...`\n",
    "#     judge_model=\"gpt-5-mini\",\n",
    "#     save_judge_jsonl=True,\n",
    "# )\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859eab38-1347-4324-be36-8768bf00c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
